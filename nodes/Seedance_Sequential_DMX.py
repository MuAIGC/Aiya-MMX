"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          Seedance è¶…é•¿è§†é¢‘è¿ç»­ç”ŸæˆèŠ‚ç‚¹ - å¤šæ¨¡æ€è§†è§‰åˆ†æç‰ˆ                    â•‘
â•‘  åŸç†ï¼šLLMçœ‹å›¾â†’æå–è§†è§‰ä¸€è‡´æ€§â†’æ™ºèƒ½ timeline åˆ‡åˆ†â†’é¦–å°¾å¸§æ¥åŠ›â†’FFmpegåˆå¹¶       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ å·¥ä½œæµç¨‹ï¼š
   1. LLM è§†è§‰åˆ†æé¦–å¸§å›¾ï¼šæå–äººç‰©æœè£…ã€å¤–è²Œã€åœºæ™¯é£æ ¼ã€æ°”è´¨éŸ³è‰²
   2. ç»“åˆç”¨æˆ·æç¤ºè¯ï¼ˆæ”¯æŒæ•…éšœè‰ºæœ¯/ç©¿æ¨¡/æ‰å¸§ç­‰ç‰¹æ®Šé£æ ¼ï¼‰ï¼Œç”Ÿæˆæ—¶é—´çº¿åˆ†é•œ
   3. æ¯æ®µæç¤ºè¯å¼ºåˆ¶é”å®šï¼šè§†è§‰ä¸€è‡´æ€§ + å£°éŸ³ä¸€è‡´æ€§ + å½“å‰æ—¶æ®µå‰§æƒ…
   4. é“¾å¼ç”Ÿæˆï¼šæ®µ1å°¾å¸§â†’æ®µ2é¦–å¸§... FFmpegæ— æŸåˆå¹¶
"""

from __future__ import annotations
import os
import io
import re
import json
import time
import uuid
import base64
import random
import shutil
import subprocess
import requests
import cv2
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from typing import List, Tuple, Optional, Dict

import folder_paths
from ..register import register_node

DEFAULT_API_URL = "https://www.dmxapi.cn"
SEEDANCE_MODEL = "doubao-seedance-1-5-pro-responses"
QUERY_MODEL = "seedance-get"
MAX_SEED = 4294967295

class Video:
    __slots__ = ("path", "_fps", "width", "height")
    def __init__(self, path: str, fps: float, width: int, height: int):
        self.path = path; self._fps = fps; self.width = width; self.height = height
    @property
    def fps(self): return self._fps
    def get_dimensions(self): return (self.width, self.height)
    def save_to(self, dst: str | Path, **kw):
        shutil.copy2(self.path, dst); return True
    def __repr__(self): return f"Video({self.path} {self._fps:.2f}fps {self.width}x{self.height})"


def image_to_base64(img_tensor) -> str:
    """ComfyUI tensor â†’ base64 data URL"""
    if img_tensor is None: raise ValueError("è¾“å…¥å›¾åƒä¸ºç©º")
    img = img_tensor[0] if img_tensor.dim() == 4 else img_tensor
    img = (img * 255).clamp(0, 255).byte().cpu().numpy()
    pil_img = Image.fromarray(img).convert("RGB")
    buffer = io.BytesIO()
    quality = 95
    while True:
        buffer.seek(0); buffer.truncate()
        pil_img.save(buffer, format="JPEG", quality=quality, optimize=True)
        if buffer.tell() < 19 * 1024 * 1024 or quality <= 10: break
        quality -= 5
    b64 = base64.b64encode(buffer.getvalue()).decode()
    return f"data:image/jpeg;base64,{b64}"


def tensor_from_pil(pil_img: Image.Image) -> torch.Tensor:
    img = np.array(pil_img.convert("RGB")).astype(np.float32) / 255.0
    tensor = torch.from_numpy(img)
    if tensor.dim() == 3: tensor = tensor.unsqueeze(0)
    return tensor


def extract_video_last_frame(video_path: Path) -> torch.Tensor:
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened(): raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames <= 0: cap.release(); raise RuntimeError("è§†é¢‘å¸§æ•°ä¸º0")
    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
    ret, frame = cap.read()
    cap.release()
    if not ret or frame is None: raise RuntimeError("æ— æ³•è¯»å–è§†é¢‘æœ€åä¸€å¸§")
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    return tensor_from_pil(Image.fromarray(frame_rgb))


def merge_videos_ffmpeg(video_paths: List[Path], output_path: Path) -> bool:
    try:
        list_file = output_path.parent / f"concat_list_{uuid.uuid4().hex[:8]}.txt"
        with open(list_file, "w", encoding="utf-8") as f:
            for vp in video_paths:
                path_str = str(vp).replace("\\", "/").replace("'", "'\\''")
                f.write(f"file '{path_str}'\n")
        cmd = [
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", str(list_file), "-c", "copy", "-movflags", "+faststart",
            str(output_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        list_file.unlink(missing_ok=True)
        if result.returncode != 0:
            print(f"[FFmpeg] å¤±è´¥: {result.stderr[:200]}"); return False
        print(f"[FFmpeg] åˆå¹¶æˆåŠŸï¼ˆå«éŸ³é¢‘ï¼‰: {output_path.name}"); return True
    except FileNotFoundError:
        print("[FFmpeg] æœªæ‰¾åˆ°ï¼Œå›é€€åˆ° OpenCV"); return False
    except Exception as e:
        print(f"[FFmpeg] å¼‚å¸¸: {e}"); return False


def merge_videos_opencv(video_paths: List[Path], output_path: Path):
    if not video_paths: raise ValueError("è§†é¢‘è·¯å¾„ä¸ºç©º")
    first_cap = cv2.VideoCapture(str(video_paths[0]))
    if not first_cap.isOpened(): raise RuntimeError(f"æ— æ³•æ‰“å¼€: {video_paths[0]}")
    ref_w = int(first_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    ref_h = int(first_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    ref_fps = first_cap.get(cv2.CAP_PROP_FPS) or 25.0
    first_cap.release()
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(str(output_path), fourcc, ref_fps, (ref_w, ref_h))
    if not writer.isOpened(): raise RuntimeError("VideoWriter å¤±è´¥")
    total_frames = 0
    try:
        for idx, vp in enumerate(video_paths):
            cap = cv2.VideoCapture(str(vp))
            if not cap.isOpened(): print(f"[OpenCV] è·³è¿‡: {vp}"); continue
            curr_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            curr_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            need_resize = (curr_w != ref_w) or (curr_h != ref_h)
            while True:
                ret, frame = cap.read()
                if not ret: break
                if need_resize: frame = cv2.resize(frame, (ref_w, ref_h), interpolation=cv2.INTER_LANCZOS4)
                writer.write(frame); total_frames += 1
            cap.release()
            print(f"[OpenCV] ç¬¬ {idx+1}æ®µå®Œæˆ")
    finally: writer.release()
    if total_frames == 0: raise RuntimeError("æ— å¸§å†™å…¥")
    print(f"[OpenCV] åˆå¹¶å®Œæˆï¼ˆæ— å£°ï¼‰: {output_path.name}")


def merge_videos(video_paths: List[Path], output_path: Path):
    if len(video_paths) == 1: shutil.copy2(video_paths[0], output_path); return
    if merge_videos_ffmpeg(video_paths, output_path): return
    print("[Merge] å›é€€åˆ° OpenCV..."); merge_videos_opencv(video_paths, output_path)


def build_video_obj(video_path: Path) -> Video:
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()
    return Video(str(video_path), fps, w, h)


class StoryboardLLM:
    """å¤šæ¨¡æ€åˆ†é•œå¯¼æ¼”ï¼šå…ˆçœ‹å›¾ï¼Œå†åˆ‡åˆ†æ—¶é—´çº¿"""
    
    def __init__(self, api_url: str, api_key: str, model: str = "gpt-4o"):
        self.api_url = api_url.rstrip("/")
        self.api_key = api_key
        self.model = model
        
    def calculate_segments(self, total_seconds: int) -> List[int]:
        """è®¡ç®—åˆ†æ®µï¼šå°½é‡10sï¼Œä½™æ•°5s"""
        segments = []
        remaining = total_seconds
        while remaining > 0:
            if remaining >= 10: segments.append(10); remaining -= 10
            elif remaining >= 5: segments.append(5); remaining -= 5
            else: segments.append(5); remaining = 0
        return segments
    
    def analyze_and_split(self, 
                          user_prompt: str, 
                          total_seconds: int,
                          first_frame_b64: str,
                          system_override: Optional[str] = None) -> Tuple[List[Tuple[int, str]], Dict, str]:
        """
        å¤šæ¨¡æ€åˆ†æï¼šçœ‹å›¾ + è¯»æç¤ºè¯ â†’ æå–ä¸€è‡´æ€§ â†’ æ—¶é—´çº¿åˆ‡åˆ†
        è¿”å›: (segments, analysis_dict, raw_llm_output)
        """
        segments = self.calculate_segments(total_seconds)
        n_segments = len(segments)
        
        # æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆå›¾+æ–‡ï¼‰
        user_content = [
            {
                "type": "image_url",
                "image_url": {"url": first_frame_b64, "detail": "high"}
            },
            {
                "type": "text",
                "text": f"""åˆ†æè¿™å¼ é¦–å¸§å›¾ï¼Œå¹¶ç»“åˆä»¥ä¸‹åˆ›æ„æè¿°ï¼Œç”Ÿæˆ{total_seconds}ç§’è§†é¢‘çš„åˆ†é•œè„šæœ¬ï¼ˆå…±{n_segments}æ®µï¼‰ã€‚

ã€ç”¨æˆ·åˆ›æ„æè¿°ã€‘ï¼š
{user_prompt}

ã€ç‰¹æ®Šé£æ ¼è¯´æ˜ã€‘
å¦‚æœæç¤ºè¯åŒ…å«"ç©¿æ¨¡"ã€"æ‰å¸§"ã€"è´´å›¾é”™è¯¯"ã€"æ‰‹Bug"ç­‰è¯æ±‡ï¼Œè¿™æ˜¯æ•…æ„çš„"æ•…éšœè‰ºæœ¯"ï¼ˆGlitch Artï¼‰é£æ ¼ï¼Œä¸æ˜¯çœŸçš„æ¸²æŸ“é”™è¯¯ã€‚è¯·åœ¨åˆ†é•œä¸­å»¶ç»­è¿™ç§"AIææ€–è°·"ç¾å­¦ã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. å…ˆè¯¦ç»†æè¿°å›¾ä¸­çš„äººç‰©å¤–è²Œã€æœè£…ã€ç¯å¢ƒå…‰çº¿ï¼ˆæå–è§†è§‰ä¸€è‡´æ€§ï¼‰
2. åˆ†æäººç‰©æ€§åˆ«å¹´é¾„ï¼Œæ¨æ–­åº”è¯¥åŒ¹é…çš„éŸ³è‰²ç‰¹å¾ï¼ˆæå–å£°éŸ³ä¸€è‡´æ€§ï¼‰
3. å°†ç”¨æˆ·åˆ›æ„æŒ‰æ—¶é—´çº¿åˆ‡åˆ†ä¸º{n_segments}æ®µï¼Œæ¯æ®µå¿…é¡»æ˜¯å‰§æƒ…çš„å»¶ç»­ï¼Œä¸¥ç¦é‡å¤
4. æ¯æ®µæç¤ºè¯å¿…é¡»åŒ…å«ï¼šä¸€è‡´æ€§é”å®š + è¯¥æ—¶æ®µçš„å…·ä½“ç”»é¢ + è¿é•œ

ã€è¾“å‡ºæ ¼å¼ - ä¸¥æ ¼JSONã€‘
{{
  "visual_analysis": {{
    "person": "äººç‰©è¯¦ç»†å¤–è²Œæœè£…",
    "environment": "åœºæ™¯å…‰çº¿è‰²è°ƒ",
    "style": "ç”»é¢é£æ ¼å…³é”®è¯"
  }},
  "audio_analysis": {{
    "voice_type": "éŸ³è‰²æè¿°ï¼ˆå¦‚å¹´è½»å¥³æ€§æ¸…è„†å£°çº¿ï¼‰",
    "speaking_style": "è¯­é€Ÿè¯­æ°”ï¼ˆå¦‚è¿ç ç‚®æœºæ¢°éŸ³ï¼‰"
  }},
  "consistency_lock": "æ€»ç»“æ‰€æœ‰å¿…é¡»ä¿æŒä¸€è‡´çš„è§†è§‰å’Œå£°éŸ³è¦ç´ ",
  "timeline": [
    {{"segment_id": 1, "time_range": "0-10s", "prompt": "ç¬¬1æ®µç‹¬æœ‰çš„ç”»é¢æè¿°ï¼ŒåŒ…å«æ•…éšœè‰ºæœ¯ç»†èŠ‚"}},
    {{"segment_id": 2, "time_range": "10-20s", "prompt": "ç¬¬2æ®µå…¨æ–°çš„ç”»é¢ï¼Œæ¥ç»­å‰æ®µå‰§æƒ…"}}
  ]
}}"""
            }
        ]
        
        default_system = """ä½ æ˜¯ä¸“ä¸šçš„AIè§†é¢‘åˆ†é•œå¯¼æ¼”ï¼Œæ“…é•¿å¤šæ¨¡æ€è§†è§‰åˆ†æã€‚
è§„åˆ™ï¼š
1. å¿…é¡»ä¸¥æ ¼æŒ‰æ—¶é—´çº¿æ¨è¿›ï¼Œç¬¬2æ®µç»å¯¹ä¸èƒ½é‡å¤ç¬¬1æ®µå†…å®¹
2. æ•…éšœè‰ºæœ¯é£æ ¼ï¼ˆç©¿æ¨¡/æ‰å¸§ï¼‰æ˜¯featureä¸æ˜¯bugï¼Œè¦å»¶ç»­
3. ç”»é¢ä¸¥ç¦å‡ºç°æ–‡å­—ã€å­—å¹•ã€UIå…ƒç´ 
4. åªè¾“å‡ºJSONï¼Œä¸è¦ä»»ä½•è§£é‡Š"""

        system_msg = system_override if system_override else default_system
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        raw_content = ""
        try:
            resp = requests.post(
                f"{self.api_url}/v1/chat/completions",
                headers=headers,
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": user_content}
                    ],
                    "temperature": 0.4,
                    "max_tokens": 3000,
                    "response_format": {"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
                },
                timeout=90
            )
            resp.raise_for_status()
            raw_content = resp.json()["choices"][0]["message"]["content"]
            print(f"[LLM Vision Raw]\n{raw_content[:800]}...")
            
            data = json.loads(raw_content)
            
            # æå–åˆ†æç»“æœ
            analysis = {
                "visual": data.get("visual_analysis", {}),
                "audio": data.get("audio_analysis", {}),
                "consistency": data.get("consistency_lock", "")
            }
            
            # æå–æ—¶é—´çº¿
            timeline = data.get("timeline", [])
            if not timeline or len(timeline) < n_segments:
                raise ValueError(f"åˆ†é•œæ•°é‡ä¸è¶³ï¼ŒæœŸæœ›{n_segments}ï¼Œå®é™…{len(timeline) if timeline else 0}")
            
            # æ„é€ æœ€ç»ˆsegments
            result = []
            consistency_text = analysis["consistency"]
            
            for i, item in enumerate(timeline[:n_segments]):
                duration = segments[i]
                seg_prompt = item.get("prompt", "")
                time_range = item.get("time_range", f"{i*10}-{(i+1)*10}s")
                
                # å¼ºåˆ¶æ³¨å…¥ä¸€è‡´æ€§æè¿°
                if consistency_text and consistency_text not in seg_prompt:
                    seg_prompt = f"{consistency_text}ã€‚{seg_prompt}"
                
                # å¼ºåˆ¶æ—¶é—´çº¿æ ‡è®°
                seg_prompt = f"[{time_range}|Segment {i+1}/{n_segments}] {seg_prompt}"
                
                # æ¸…ç†æ±¡æŸ“è¯
                seg_prompt = re.sub(r'\b(å­—å¹•|æ–‡å­—|text|subtitle|caption|é‡å¤|again)\b', '', seg_prompt, flags=re.IGNORECASE)
                
                result.append((duration, seg_prompt))
            
            return result, analysis, raw_content
            
        except Exception as e:
            print(f"[StoryboardLLM] è§†è§‰åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨æ–‡æœ¬å¼ºåˆ¶åˆ‡åˆ†")
            # Fallbackï¼šæŒ‰å¥å­æ•°åˆ‡åˆ†
            return self._fallback_split(user_prompt, segments), {}, raw_content + f"\n[Error: {e}]"
    
    def _fallback_split(self, user_prompt: str, segments: List[int]) -> List[Tuple[int, str]]:
        """æ–‡æœ¬å¼ºåˆ¶åˆ‡åˆ†ï¼šæŒ‰é•¿åº¦å‡åŒ€åˆ†é…"""
        # ç®€å•æŒ‰é€—å·/å¥å·åˆ‡åˆ†
        parts = [p.strip() for p in re.split(r'[ã€‚ï¼Œ,ï¼›;ï¼!ï¼Ÿ?]', user_prompt) if p.strip()]
        total_parts = len(parts)
        lines_per_seg = max(1, total_parts // len(segments))
        
        result = []
        for i, duration in enumerate(segments):
            start = i * lines_per_seg
            end = start + lines_per_seg if i < len(segments) - 1 else total_parts
            seg_text = "ï¼Œ".join(parts[start:end]) if parts else user_prompt
            seg_text = f"[å¼ºåˆ¶åˆ‡åˆ†|ç¬¬{i+1}æ®µ] {seg_text}"
            result.append((duration, seg_text))
        
        return result


class SeedanceSequentialVideo:
    """ğŸ¬ å¤šæ¨¡æ€è¶…é•¿è§†é¢‘ç”Ÿæˆï¼šLLMçœ‹å›¾â†’æ™ºèƒ½åˆ†é•œâ†’é“¾å¼ç”Ÿæˆ"""
    
    DESCRIPTION = """
ğŸ’¡ å“å‘€âœ¦Seedance è§†è§‰åˆ†æç‰ˆ â€”â€” LLMå…ˆçœ‹é¦–å¸§å›¾ï¼Œå†ç”Ÿæˆä¸€è‡´æ€§åˆ†é•œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… å·¥ä½œæµç¨‹ï¼š
   1. å¤šæ¨¡æ€åˆ†æï¼šLLMè¯†åˆ«é¦–å¸§å›¾ä¸­çš„äººç‰©æœè£…/å¤–è²Œ/ç¯å¢ƒ/æ°”è´¨
   2. æ™ºèƒ½åˆ†é•œï¼šç»“åˆç”¨æˆ·æç¤ºè¯ï¼ˆæ”¯æŒæ•…éšœè‰ºæœ¯é£æ ¼ï¼‰ï¼ŒæŒ‰æ—¶é—´çº¿åˆ‡åˆ†
   3. ä¸€è‡´æ€§é”å®šï¼šè§†è§‰+å£°éŸ³ç‰¹å¾åœ¨æ‰€æœ‰åˆ†é•œä¸­å¼ºåˆ¶å¤ç°
   4. é“¾å¼ç”Ÿæˆï¼šå°¾å¸§æ¥åŠ›ï¼ŒFFmpegæ— æŸåˆå¹¶

âš ï¸ æç¤ºï¼š
   â€¢ æ”¯æŒ"ç©¿æ¨¡/æ‰å¸§/è´´å›¾é”™è¯¯"ç­‰æ•…éšœè‰ºæœ¯é£æ ¼ï¼ŒLLMä¼šç†è§£è¿™æ˜¯ intentional glitch
   â€¢ è‹¥LLM visionä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ°æ–‡æœ¬åˆ‡åˆ†
    """
    
    RETURN_TYPES = ("VIDEO", "STRING", "STRING", "STRING", "IMAGE")
    RETURN_NAMES = ("video", "segments_info", "final_url", "llm_analysis", "preview_last_frame")
    FUNCTION = "generate_sequence"
    CATEGORY = "å“å‘€âœ¦MMX/DMXAPI"
    
    @classmethod
    def INPUT_TYPES(cls):
        default_system = """ä½ æ˜¯ä¸“ä¸šçš„AIè§†é¢‘åˆ†é•œå¯¼æ¼”ï¼Œæ“…é•¿åˆ†æå›¾åƒå¹¶ç”Ÿæˆæ—¶é—´çº¿åˆ†é•œã€‚
æ ¸å¿ƒè§„åˆ™ï¼š
1. å…ˆè¯¦ç»†æè¿°é¦–å¸§å›¾çš„è§†è§‰ä¿¡æ¯ï¼ˆäººç‰©æœè£…ã€å¤–è²Œã€å…‰çº¿ï¼‰
2. åˆ†æäººç‰©æ°”è´¨ï¼Œç¡®å®šéŸ³è‰²ç‰¹å¾ï¼ˆæ€§åˆ«ã€å¹´é¾„ã€è¯­é€Ÿï¼‰
3. å°†å‰§æƒ…æŒ‰æ—¶é—´çº¿æ¨è¿›ï¼Œç¬¬Næ®µå¿…é¡»æ˜¯ç¬¬N-1æ®µçš„å»¶ç»­ï¼Œç»å¯¹ç¦æ­¢é‡å¤
4. æ”¯æŒæ•…éšœè‰ºæœ¯ï¼ˆGlitch Artï¼‰ï¼šç©¿æ¨¡ã€æ‰å¸§ã€è´´å›¾é”™è¯¯æ˜¯ç¾çš„è¡¨è¾¾ï¼Œä¸æ˜¯Bug
5. è¾“å‡ºä¸¥æ ¼JSONï¼Œç”»é¢ä¸¥ç¦æ–‡å­—/å­—å¹•"""
        
        return {
            "required": {
                "api_url": ("STRING", {"default": DEFAULT_API_URL, "multiline": False}),
                "api_key": ("STRING", {"default": "sk-", "multiline": False}),
                "first_frame": ("IMAGE",),
                "total_duration_sec": ("INT", {"default": 20, "min": 10, "max": 120, "step": 10}),
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "äººç‰©è‡ªä¿¡é€¼è¿‘é•œå¤´ï¼Œæ•…éšœè‰ºæœ¯é£æ ¼ï¼Œç©¿æ¨¡æ‰å¸§",
                    "placeholder": "æè¿°å‰§æƒ…ï¼Œæ”¯æŒæ•…éšœè‰ºæœ¯/ç©¿æ¨¡/æ‰å¸§ç­‰ç‰¹æ®Šé£æ ¼..."
                }),
                "system_prompt": ("STRING", {
                    "default": default_system,
                    "multiline": True,
                    "placeholder": "å¤šæ¨¡æ€åˆ†æç³»ç»Ÿæç¤ºè¯..."
                }),
                "resolution": (["480p", "720p", "1080p"], {"default": "720p"}),
                "ratio": (["16:9", "9:16", "1:1", "4:3", "3:4", "21:9"], {"default": "9:16"}),
                "llm_model": ("STRING", {"default": "gpt-4o", "multiline": False}),
            },
            "optional": {
                "seed": ("INT", {"default": -1, "min": -1, "max": MAX_SEED}),
                "generate_audio": (["å¼€å¯", "å…³é—­"], {"default": "å¼€å¯"}),
                "watermark": (["æ— ", "æ·»åŠ "], {"default": "æ— "}),
            }
        }
    
    def submit_segment(self, api_url: str, token: str, prompt: str, 
                       first_frame_b64: str, resolution: str, ratio: str, 
                       duration: int, seed: int, generate_audio: bool, 
                       watermark: bool) -> str:
        ratio_clean = ratio
        input_arr = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": first_frame_b64}, "role": "first_frame"}
        ]
        actual_seed = random.randint(0, MAX_SEED) if seed == -1 else seed
        if actual_seed > MAX_SEED: actual_seed = actual_seed % (MAX_SEED + 1)
        payload = {
            "model": SEEDANCE_MODEL,
            "input": input_arr,
            "callback_url": "",
            "return_last_frame": False,
            "generate_audio": generate_audio,
            "resolution": resolution,
            "ratio": ratio_clean,
            "duration": duration,
            "seed": actual_seed,
            "camera_fixed": False,
            "watermark": watermark
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token.strip()}"
        }
        url = f"{api_url.rstrip('/')}/v1/responses"
        resp = requests.post(url, json=payload, headers=headers, timeout=30)
        resp.raise_for_status()
        result = resp.json()
        if "id" not in result: raise RuntimeError(f"æäº¤å¤±è´¥: {result}")
        return result["id"]
    
    def query_segment(self, api_url: str, task_id: str, token: str) -> str:
        url = f"{api_url.rstrip('/')}/v1/responses"
        payload = {"model": QUERY_MODEL, "input": task_id, "stream": True}
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {token.strip()}"}
        video_url = None
        with requests.post(url, json=payload, headers=headers, stream=True, timeout=180) as resp:
            resp.raise_for_status()
            for line in resp.iter_lines():
                if not line: continue
                line_str = line.decode('utf-8').strip()
                if line_str.startswith('data: '): line_str = line_str[6:]
                try:
                    data = json.loads(line_str)
                    if data.get('type') == "response.completed":
                        text = data.get('response', {}).get('output', [{}])[0].get('content', [{}])[0].get('text', '')
                        matches = re.findall(r'(https://[^\s\n\"]+(?:\.mp4|\.mov)[^\s\n\"]*)', text)
                        if matches: video_url = matches[0].rstrip('.,;')
                except: continue
        if not video_url: raise RuntimeError("æœªè·å–è§†é¢‘URL")
        return video_url
    
    def download_segment(self, url: str, save_path: Path):
        for attempt in range(3):
            try:
                with requests.get(url, stream=True, timeout=120) as r:
                    r.raise_for_status()
                    with open(save_path, "wb") as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk: f.write(chunk)
                return
            except Exception as e:
                print(f"[Download] ç¬¬{attempt+1}æ¬¡å¤±è´¥: {e}")
                if attempt == 2: raise
                time.sleep(2 ** attempt)
    
    def generate_sequence(self, api_url, api_key, first_frame, total_duration_sec, 
                         prompt, system_prompt, resolution, ratio, llm_model,
                         seed=-1, generate_audio="å¼€å¯", watermark="æ— "):
        
        token = api_key.strip()
        if not token or token == "sk-": raise RuntimeError("API Key æ— æ•ˆ")
        
        output_dir = Path(folder_paths.get_output_directory()) / "seedance_sequential"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. å‡†å¤‡é¦–å¸§ base64
        first_frame_b64 = image_to_base64(first_frame)
        
        # 2. LLM å¤šæ¨¡æ€åˆ†æ + åˆ†é•œ
        print(f"[Sequential] å¤šæ¨¡æ€åˆ†æé¦–å¸§å›¾ + ç”Ÿæˆ {total_duration_sec}s åˆ†é•œ...")
        print(f"[Sequential] ä½¿ç”¨æ¨¡å‹: {llm_model}")
        
        llm = StoryboardLLM(api_url, token, llm_model)
        segments, analysis, raw_llm = llm.analyze_and_split(
            prompt, total_duration_sec, first_frame_b64, system_prompt
        )
        
        segment_count = len(segments)
        print(f"[Sequential] åˆ†é•œå®Œæˆ: {segment_count} æ®µ")
        
        # æ‰“å°åˆ†æç»“æœ
        if analysis:
            print(f"  [è§†è§‰åˆ†æ] {analysis.get('visual', {}).get('person', 'N/A')[:50]}...")
            print(f"  [å£°éŸ³é”å®š] {analysis.get('audio', {}).get('voice_type', 'N/A')}")
        
        for i, (d, p) in enumerate(segments, 1):
            print(f"  æ®µ{i} ({d}s): {p[:70]}...")
        
        # 3. é€æ®µç”Ÿæˆ
        video_files = []
        current_frame = first_frame
        segment_infos = []
        last_frame_tensor = None
        
        for idx, (duration, seg_prompt) in enumerate(segments, 1):
            print(f"\n[Sequential] ç”Ÿæˆç¬¬ {idx}/{segment_count} æ®µï¼ˆ{duration}sï¼‰...")
            
            try:
                frame_b64 = image_to_base64(current_frame)
                seg_seed = seed + idx - 1 if seed != -1 else -1
                
                task_id = self.submit_segment(
                    api_url, token, seg_prompt, frame_b64,
                    resolution, ratio, duration,
                    seg_seed,
                    generate_audio == "å¼€å¯",
                    watermark == "æ·»åŠ "
                )
                
                video_url = self.query_segment(api_url, task_id, token)
                
                video_path = output_dir / f"seq_{idx:03d}_{uuid.uuid4().hex[:6]}.mp4"
                self.download_segment(video_url, video_path)
                video_files.append(video_path)
                
                segment_infos.append({
                    "segment": idx,
                    "duration": duration,
                    "prompt": seg_prompt,
                    "video_file": str(video_path.name),
                    "url": video_url
                })
                
                # æŠ½å–å°¾å¸§ä½œä¸ºä¸‹ä¸€æ®µé¦–å¸§
                if idx < segment_count:
                    print(f"[Sequential] æŠ½å–å°¾å¸§...")
                    last_frame_tensor = extract_video_last_frame(video_path)
                    current_frame = last_frame_tensor
                else:
                    # æœ€åä¸€æ®µä¹ŸæŠ½å¸§ï¼Œä½œä¸ºé¢„è§ˆè¾“å‡º
                    last_frame_tensor = extract_video_last_frame(video_path)
                    
            except Exception as e:
                print(f"[Sequential] ç¬¬ {idx} æ®µå¤±è´¥: {e}")
                info_path = output_dir / f"failed_at_seg_{idx}.json"
                with open(info_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "failed_at": idx, "error": str(e), 
                        "completed": [str(p) for p in video_files],
                        "llm_raw": raw_llm
                    }, f, ensure_ascii=False, indent=2)
                raise RuntimeError(f"ç¬¬ {idx} æ®µå¤±è´¥: {e}")
        
        # 4. åˆå¹¶
        final_name = f"seedance_long_{total_duration_sec}s_{uuid.uuid4().hex[:8]}.mp4"
        final_video = output_dir / final_name
        
        if len(video_files) == 1:
            shutil.copy2(video_files[0], final_video)
        else:
            print(f"[Sequential] åˆå¹¶ {len(video_files)} æ®µ...")
            merge_videos(video_files, final_video)
        
        # 5. è¾“å‡º
        video_obj = build_video_obj(final_video)
        info_text = json.dumps({
            "total_duration": total_duration_sec,
            "segments_count": segment_count,
            "final_video": str(final_video),
            "llm_model": llm_model,
            "segments": segment_infos,
            "analysis": analysis
        }, ensure_ascii=False, indent=2)
        
        final_url = segment_infos[-1]["url"] if segment_infos else ""
        analysis_text = json.dumps(analysis, ensure_ascii=False, indent=2) if analysis else raw_llm
        
        return (video_obj, info_text, final_url, analysis_text, last_frame_tensor)


register_node(SeedanceSequentialVideo, "Seedance15Pro-è¶…é•¿è§†é¢‘ç”Ÿæˆ-DMX")